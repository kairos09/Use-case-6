GPT-4
User
Create a Python Script that would collect the following metrics from AWS Cloud: number of unencrypted EBS volumes and their storage size, number of unattached EBS volumes and their storage size, number of unencrypted EBS snapshots and their size and put the outputs in a JSON file. This script will be run in AWS Lambda
ChatGPT

Here's an example Python script that uses the boto3 library to fetch the desired metrics from AWS. It collects information about unencrypted and unattached EBS volumes, unencrypted EBS snapshots, and their sizes. This information is then written to a JSON file. Remember to set your region and AWS credentials properly.

Please note that this script assumes that unattached volumes are those without any attachments. In some cases, a volume can have an attachment but the instance it's attached to is terminated, you might want to refine the conditions based on your specific requirements.

This script should be compatible with AWS Lambda with the boto3 module.

python

import boto3
import json

def lambda_handler(event, context):

    ec2 = boto3.resource('ec2')
    unencrypted_volumes = []
    unattached_volumes = []
    unencrypted_snapshots = []

    for volume in ec2.volumes.all():
        if volume.encrypted == False:
            unencrypted_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})
        if len(volume.attachments) == 0:
            unattached_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})

    for snapshot in ec2.snapshots.filter(OwnerIds=['self']):
        if snapshot.encrypted == False:
            unencrypted_snapshots.append({"SnapshotId": snapshot.snapshot_id, "Size": snapshot.volume_size})

    result = {
        'UnencryptedVolumes': unencrypted_volumes,
        'UnattachedVolumes': unattached_volumes,
        'UnencryptedSnapshots': unencrypted_snapshots
    }

    with open('/tmp/results.json', 'w') as fp:
        json.dump(result, fp)

    # to read it back and print:
    with open('/tmp/results.json', 'r') as fp:
        print(json.load(fp))

# lambda_handler(None, None) # Uncomment for testing outside of AWS Lambda

Please replace 'ec2' with the correct region for your application.

One thing to note about this script is that it only scans snapshots owned by the account (using OwnerIds=['self']), so if there are other snapshots not owned by the account you may need to adjust the script.

Also, keep in mind that AWS Lambda functions have a maximum execution time (15 minutes as of my knowledge cutoff in 2021). If you have a very large number of volumes or snapshots, you may need to modify this script to paginate through the volumes and snapshots. Similarly, Lambda has a limit on local storage (512 MB as of 2021), so if the JSON results are too large, you may need to offload the data to an S3 bucket instead.

Lastly, ensure your IAM role associated with the Lambda function has necessary permissions to perform the operations like ec2:DescribeVolumes and ec2:DescribeSnapshots.
User
Adjust the script to put the JSON file in a S3 bucket. In addition generate an IAM policy that will be attached to an IAM Role that will be used as the Lambdas execution role. Give the policy permission to describe volumes and snapshots as well as permissions to view and write to the S3 bucket
ChatGPT

Sure, I will adjust the script to store the JSON file in an S3 bucket, and also provide an IAM policy that has the necessary permissions.

Here's the updated script that writes the JSON file to an S3 bucket:

python

import boto3
import json

def lambda_handler(event, context):

    ec2 = boto3.resource('ec2')
    s3 = boto3.resource('s3')
    bucket_name = "your_bucket_name"  # replace with your bucket name
    unencrypted_volumes = []
    unattached_volumes = []
    unencrypted_snapshots = []

    for volume in ec2.volumes.all():
        if volume.encrypted == False:
            unencrypted_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})
        if len(volume.attachments) == 0:
            unattached_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})

    for snapshot in ec2.snapshots.filter(OwnerIds=['self']):
        if snapshot.encrypted == False:
            unencrypted_snapshots.append({"SnapshotId": snapshot.snapshot_id, "Size": snapshot.volume_size})

    result = {
        'UnencryptedVolumes': unencrypted_volumes,
        'UnattachedVolumes': unattached_volumes,
        'UnencryptedSnapshots': unencrypted_snapshots
    }

    s3.Object(bucket_name, 'results.json').put(Body=json.dumps(result))

For the IAM policy, it will look something like this. You will have to replace 'your_bucket_name' with the actual name of your S3 bucket. This policy will allow the role to describe EC2 volumes and snapshots and to read/write from/to the S3 bucket:

json

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeVolumes",
                "ec2:DescribeSnapshots"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:PutObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::your_bucket_name",
                "arn:aws:s3:::your_bucket_name/*"
            ]
        }
    ]
}

To create an IAM role:

    Navigate to the IAM service in the AWS Management Console.
    Click "Roles" in the sidebar.
    Click "Create role".
    Choose "AWS service", then "Lambda". Click "Next: Permissions".
    Click "Create policy", then paste in the above policy. Review and save the policy.
    Back on the "Create role" screen, refresh and select your new policy.
    Proceed to create the role. Use this new role when you create your Lambda function.

Please keep in mind AWS Policy best practices, like granting least privilege, i.e., only granting the permissions required to perform a task. Adjust the provided IAM policy as per your security requirements.
User
Create a AWS EventBridge schedule that would run the Lambda UseCase6(Lambda with the code you provided) once a day. Also add exception handling to the function as well as logging. Logging and exceptions should be stored in a JSON file log.json in the same bucket as the results.json
ChatGPT

Firstly, let's update the script to include exception handling and logging. We'll use Python's built-in logging module for the logging, and exceptions are written directly to the log.json file in the S3 bucket:

python

import boto3
import json
import logging

def lambda_handler(event, context):

    # Configure logging
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    s3 = boto3.resource('s3')
    ec2 = boto3.resource('ec2')
    bucket_name = "your_bucket_name"  # replace with your bucket name

    unencrypted_volumes = []
    unattached_volumes = []
    unencrypted_snapshots = []

    try:
        for volume in ec2.volumes.all():
            if volume.encrypted == False:
                unencrypted_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})
            if len(volume.attachments) == 0:
                unattached_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})

        for snapshot in ec2.snapshots.filter(OwnerIds=['self']):
            if snapshot.encrypted == False:
                unencrypted_snapshots.append({"SnapshotId": snapshot.snapshot_id, "Size": snapshot.volume_size})

        result = {
            'UnencryptedVolumes': unencrypted_volumes,
            'UnattachedVolumes': unattached_volumes,
            'UnencryptedSnapshots': unencrypted_snapshots
        }

        s3.Object(bucket_name, 'results.json').put(Body=json.dumps(result))
        logger.info('Successfully wrote results to S3')

    except Exception as e:
        logger.error(f'Error occurred: {e}')
        s3.Object(bucket_name, 'log.json').put(Body=json.dumps({"error": str(e)}))

# lambda_handler(None, None) # Uncomment for testing outside of AWS Lambda

This code will write error messages to log.json in the same S3 bucket if an exception is thrown.

To schedule your Lambda function to run once a day using EventBridge:

    Navigate to the EventBridge service in the AWS Management Console.
    Click on "Create rule".
    Give your rule a name and description.
    Under "Define pattern", choose "Schedule".
    In the cron expression, enter 0 12 * * ? * to run the function once a day at noon. Adjust the time as necessary.
    Under "Select targets", choose "Lambda function" and select your function.
    Click "Create".

Remember that the IAM policy for the Lambda function will also need to be updated with the necessary permissions to write to CloudWatch Logs for the logging to work. Here's an addition to the policy:

json

{
    "Effect": "Allow",
    "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
    ],
    "Resource": "arn:aws:logs:*:*:*"
}

This will allow the function to create and write to log streams in CloudWatch Logs.
User
Update the IAM policy with all the changes you suggested.
ChatGPT

Sure, here is the updated IAM policy incorporating all permissions we've discussed - including the permission to describe volumes and snapshots, read/write permissions for the specified S3 bucket, and permissions to write to CloudWatch Logs:

json

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeVolumes",
                "ec2:DescribeSnapshots"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:PutObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::your_bucket_name",
                "arn:aws:s3:::your_bucket_name/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            "Resource": "arn:aws:logs:*:*:*"
        }
    ]
}

Remember to replace 'your_bucket_name' with the actual name of your S3 bucket.

Also, please ensure that your use case follows the principle of least privilege and only grants the necessary permissions to the IAM role for the tasks it needs to perform. You may need to adjust this policy based on your specific security requirements.
User
Create a deployment package with the required libraries to deploy the code you created
ChatGPT

In order to create a deployment package for AWS Lambda with the required libraries, you need to create a ZIP archive that includes your Python code (lambda_function.py, for instance) and any necessary Python libraries not included in the AWS Lambda Python runtime.

In this case, the necessary library for your code is boto3. However, boto3 is already included in the AWS Lambda Python runtime, so you do not need to package it.

Here are the steps to create a deployment package if needed:

    Write your code in a .py file (lambda_function.py).

python

# lambda_function.py

import boto3
import json
import logging

def lambda_handler(event, context):

    # Configure logging
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    s3 = boto3.resource('s3')
    ec2 = boto3.resource('ec2')
    bucket_name = "your_bucket_name"  # replace with your bucket name

    unencrypted_volumes = []
    unattached_volumes = []
    unencrypted_snapshots = []

    try:
        for volume in ec2.volumes.all():
            if volume.encrypted == False:
                unencrypted_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})
            if len(volume.attachments) == 0:
                unattached_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})

        for snapshot in ec2.snapshots.filter(OwnerIds=['self']):
            if snapshot.encrypted == False:
                unencrypted_snapshots.append({"SnapshotId": snapshot.snapshot_id, "Size": snapshot.volume_size})

        result = {
            'UnencryptedVolumes': unencrypted_volumes,
            'UnattachedVolumes': unattached_volumes,
            'UnencryptedSnapshots': unencrypted_snapshots
        }

        s3.Object(bucket_name, 'results.json').put(Body=json.dumps(result))
        logger.info('Successfully wrote results to S3')

    except Exception as e:
        logger.error(f'Error occurred: {e}')
        s3.Object(bucket_name, 'log.json').put(Body=json.dumps({"error": str(e)}))

    As the boto3 library is included in the AWS Lambda Python runtime, you don't need to create a virtual environment and add it to the package. If you had to include other libraries, you'd typically create a virtual environment, install your dependencies, and include them in your package.

    Create a ZIP archive of the lambda_function.py file.

On Unix-like systems (Linux, MacOS), you can use the zip command in the terminal:

bash

zip function.zip lambda_function.py

On Windows, you can use a GUI tool like 7-Zip or WinZip to create the ZIP archive.

    Then, upload the function.zip file to AWS Lambda.

Please remember to replace 'your_bucket_name' in the script with the actual name of your S3 bucket before creating the ZIP archive.
User
 Move the bucket_name variable to be read from Lambda environment variables. Adjust the deployment package accordingly or provide instructions on how to setup that variable for lambda.
ChatGPT

To make your script more flexible, you can read the S3 bucket name from environment variables. Here is the updated Python script:

python

# lambda_function.py

import boto3
import json
import logging
import os

def lambda_handler(event, context):

    # Configure logging
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    s3 = boto3.resource('s3')
    ec2 = boto3.resource('ec2')
    bucket_name = os.environ['BUCKET_NAME']  # read from environment variables

    unencrypted_volumes = []
    unattached_volumes = []
    unencrypted_snapshots = []

    try:
        for volume in ec2.volumes.all():
            if volume.encrypted == False:
                unencrypted_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})
            if len(volume.attachments) == 0:
                unattached_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})

        for snapshot in ec2.snapshots.filter(OwnerIds=['self']):
            if snapshot.encrypted == False:
                unencrypted_snapshots.append({"SnapshotId": snapshot.snapshot_id, "Size": snapshot.volume_size})

        result = {
            'UnencryptedVolumes': unencrypted_volumes,
            'UnattachedVolumes': unattached_volumes,
            'UnencryptedSnapshots': unencrypted_snapshots
        }

        s3.Object(bucket_name, 'results.json').put(Body=json.dumps(result))
        logger.info('Successfully wrote results to S3')

    except Exception as e:
        logger.error(f'Error occurred: {e}')
        s3.Object(bucket_name, 'log.json').put(Body=json.dumps({"error": str(e)}))

Creating a deployment package remains the same as described in the previous step.

To set the BUCKET_NAME environment variable for your Lambda function, follow these steps:

    Navigate to the AWS Lambda service in the AWS Management Console.
    Click on your function.
    In the function's configuration page, under the "Configuration" tab, click on "Environment variables".
    Click "Add environment variable".
    Set "Key" to BUCKET_NAME and "Value" to the name of your S3 bucket.
    Click "Save" at the top right of the page.

Remember that the bucket name is sensitive information. Always use secure ways to store and access such information, and limit who can see or change it.
User
Now create a final deployment package with the rest of required libraries(os, logging, json)  and the code you included.
ChatGPT

While creating a deployment package for AWS Lambda, we only need to package the libraries that are not part of the AWS Lambda Python runtime. As of my knowledge cutoff in September 2021, the os, logging, and json libraries, along with boto3, are included in the AWS Lambda Python runtime. Therefore, you don't need to include these in your deployment package.

The deployment package for this AWS Lambda function will only contain your Python script. Here are the steps to create your deployment package:

    Write your code in a .py file (e.g., lambda_function.py).

python

# lambda_function.py

import boto3
import json
import logging
import os

def lambda_handler(event, context):

    # Configure logging
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    s3 = boto3.resource('s3')
    ec2 = boto3.resource('ec2')
    bucket_name = os.environ['BUCKET_NAME']  # read from environment variables

    unencrypted_volumes = []
    unattached_volumes = []
    unencrypted_snapshots = []

    try:
        for volume in ec2.volumes.all():
            if volume.encrypted == False:
                unencrypted_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})
            if len(volume.attachments) == 0:
                unattached_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})

        for snapshot in ec2.snapshots.filter(OwnerIds=['self']):
            if snapshot.encrypted == False:
                unencrypted_snapshots.append({"SnapshotId": snapshot.snapshot_id, "Size": snapshot.volume_size})

        result = {
            'UnencryptedVolumes': unencrypted_volumes,
            'UnattachedVolumes': unattached_volumes,
            'UnencryptedSnapshots': unencrypted_snapshots
        }

        s3.Object(bucket_name, 'results.json').put(Body=json.dumps(result))
        logger.info('Successfully wrote results to S3')

    except Exception as e:
        logger.error(f'Error occurred: {e}')
        s3.Object(bucket_name, 'log.json').put(Body=json.dumps({"error": str(e)}))

    Create a ZIP archive of the lambda_function.py file.

On Unix-like systems (Linux, MacOS), you can use the zip command in the terminal:

bash

zip function.zip lambda_function.py

On Windows, you can use a GUI tool like 7-Zip or WinZip to create the ZIP archive.

    Then, upload the function.zip file to AWS Lambda.

Please note that if you would have additional dependencies that are not included in the AWS Lambda Python runtime, you'd need to create a virtual environment, install these dependencies, and then add them to your deployment package. But for this particular script, the os, logging, json, and boto3 libraries are sufficient, and these are all included in the AWS Lambda Python runtime.

User
Edit the application code to scan for volumes in us-east-1 region.
ChatGPT

To restrict your script to only scan for volumes in the us-east-1 region, you simply need to specify the region when you create the ec2 resource.

Here's the adjusted code with the region specified:

python

import boto3
import json
import logging
import os

def lambda_handler(event, context):

    # Configure logging
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    s3 = boto3.resource('s3')
    ec2 = boto3.resource('ec2', region_name='us-east-1')  # specify region
    bucket_name = os.environ['BUCKET_NAME']  # read from environment variables

    unencrypted_volumes = []
    unattached_volumes = []
    unencrypted_snapshots = []

    try:
        for volume in ec2.volumes.all():
            if volume.encrypted == False:
                unencrypted_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})
            if len(volume.attachments) == 0:
                unattached_volumes.append({"VolumeId": volume.volume_id, "Size": volume.size})

        for snapshot in ec2.snapshots.filter(OwnerIds=['self']):
            if snapshot.encrypted == False:
                unencrypted_snapshots.append({"SnapshotId": snapshot.snapshot_id, "Size": snapshot.volume_size})

        result = {
            'UnencryptedVolumes': unencrypted_volumes,
            'UnattachedVolumes': unattached_volumes,
            'UnencryptedSnapshots': unencrypted_snapshots
        }

        s3.Object(bucket_name, 'results.json').put(Body=json.dumps(result))
        logger.info('Successfully wrote results to S3')

    except Exception as e:
        logger.error(f'Error occurred: {e}')
        s3.Object(bucket_name, 'log.json').put(Body=json.dumps({"error": str(e)}))


